import argparse
import re
import string

import fasttext
import nltk
import numpy as np
import tensorflow as tf
from model import factory


class Model1_IssueTracker_Li2022_ESEM:
    """
    Self-admitted technical debt verifier
    """

    name:str

    def __init__(self, weight_file, word_embedding_file):
        # Load the model and its weights
        print('Loading model {}...'.format(weight_file))
        self._model = tf.keras.models.load_model(weight_file)
        self._model.trainable = False
        self._size_of_input = self._model.layers[0].get_output_at(0).get_shape()[1]

        # Load the FastText word embeddings
        self._word_embedding = fasttext.load_model(word_embedding_file)
        self._word_embedding_cache = {}

        # Initialize the tokenizer and punctuation settings
        self._tokenizer_words = nltk.TweetTokenizer()

        # Set up label configurations
        label_num = self._model.layers[-1].get_output_at(0).get_shape()[-1]
        self._labels = ['SATD', 'non-SATD']
        self._padding = '<pad>'

    def comment_pre_processing(self, comment):
        """
        Pre-process comment

        :param comment:
        :return:
        """
        # Remove comment delimiters and convert to lowercase
        comment = re.sub('(//)|(/\\*)|(\\*/)', '', comment)
        comment = comment.replace('\ud83d', '').lower()
        # Tokenize comment into sentences and words
        tokens_sentences = [self._tokenizer_words.tokenize(t) for t in nltk.sent_tokenize(comment)]
        tokens = [word for t in tokens_sentences for word in t]
        return tokens

    def prepare_comments(self, comment):
        """
        Prepare comments for machine learning model

        :return:
        """
        # Pre-process the comment
        pre_stripped = self.comment_pre_processing(comment)

        # Pad or truncate the comment based on the input size
        if len(pre_stripped) > self._size_of_input:
            new_sentence = pre_stripped[:self._size_of_input]
        else:
            num_padding = self._size_of_input - len(pre_stripped)
            new_sentence = pre_stripped + [self._padding] * num_padding

        # Convert words to word embeddings
        x_test = []
        for word in new_sentence:
            if word not in self._word_embedding_cache:
                word_embed = self._word_embedding[word]
                self._word_embedding_cache[word] = word_embed
                x_test.append(word_embed)
            else:
                x_test.append(self._word_embedding_cache[word])
        return np.array([x_test])

    def clear_model_session(self):
        tf.keras.backend.clear_session()

    def label(self, comment):
        """
        Classify a single comment

        :param comment:
        """
        # Prepare the comment for classification
        input_x = self.prepare_comments(comment)

        # Make predictions using the model
        y_pred = self._model.predict(input_x)
        y_pred_bool = np.argmax(y_pred, axis=1)

        # Print the prediction results
        return self._labels[y_pred_bool[0]]

    def label_sections_in_batch(self, comments, batch_size):
        """
        Classify a single comment

        :param comment:
        """
        # Prepare the comment for classification
        input_x = np.concatenate([self.prepare_comments(x) for x in comments])

        # Make predictions using the model
        y_pred = self._model.predict(input_x, batch_size=batch_size, verbose=1)
        y_pred_ints = np.argmax(y_pred, axis=1)

        # Print the prediction results
        return [self._labels[y] for y in y_pred_ints]
    
def initialize() -> None:
    factory.register_model("Model1_IssueTracker_Li2022_ESEM", Model1_IssueTracker_Li2022_ESEM)